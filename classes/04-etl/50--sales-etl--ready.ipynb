{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales etl code\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "from airflow.providers.mysql.hooks.mysql import MySqlHook\n",
    "from airflow.hooks.filesystem import FSHook\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['jakub.kanclerz@gmail.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "from pprint import pprint as pp\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def load_time(**kwargs):\n",
    "    pp(kwargs['execution_date'])\n",
    "    current = kwargs['execution_date']\n",
    "    limit = current.today() + relativedelta(months=12)\n",
    "    db = PostgresHook(postgres_conn_id=\"my_warehouse_1\")\n",
    "\n",
    "    pp(db)\n",
    "    conn = db.get_conn()\n",
    "    c = conn.cursor()\n",
    "\n",
    "    c.execute('''\n",
    "        INSERT INTO datetime_dimension\n",
    "        SELECT\n",
    "            cast(to_char(ds, 'YYYYMMDDHH24') as INTEGER) as id,\n",
    "            ds,\n",
    "            EXTRACT(year from ds) as year,\n",
    "            EXTRACT(month from ds) as month,\n",
    "            EXTRACT(day from ds) as day,\n",
    "            EXTRACT(WEEK from ds) as week,\n",
    "            EXTRACT(HOUR from ds) as hour,\n",
    "            EXTRACT(dow from ds) as day_of_week\n",
    "\n",
    "        from (\n",
    "        SELECT\n",
    "            generate_series(\n",
    "                (date %(current)s)::timestamp,\n",
    "                (date %(limit)s)::timestamp,\n",
    "                interval '1 hour') as ds ) as t1\n",
    "        ON CONFLICT \n",
    "        ON CONSTRAINT datetime_dimension_pkey DO NOTHING\n",
    "    ''', {\"limit\": limit, \"current\": current})\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "def extract_products(**kwargs):\n",
    "    db = MySqlHook(mysql_conn_id='my_ecommerce')\n",
    "    pp(kwargs['execution_date'])\n",
    "\n",
    "    fs = FSHook(conn_id=\"tmp_fs\")\n",
    "    logging.info(\"FS path: {}\".format(fs.get_path()))\n",
    "   \n",
    "    file_name = str(kwargs[\"execution_date\"]) + '.json'\n",
    "    dir_path = os.path.join(\n",
    "        fs.get_path(),\n",
    "        kwargs[\"dag\"].dag_id,\n",
    "        kwargs[\"task\"].task_id)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    \n",
    "    target = os.path.join(dir_path, file_name)\n",
    "    \n",
    "\n",
    "    conn = db.get_conn()\n",
    "    c = conn.cursor()\n",
    "\n",
    "    c.execute('''\n",
    "        SELECT\n",
    "            p.id as product_id,\n",
    "            p.code as name\n",
    "        FROM\n",
    "            ecommerce_product p \n",
    "    ''')\n",
    "\n",
    "    res = c.fetchall()\n",
    "    json.dump(res, open(target, 'w+'))\n",
    "\n",
    "def load_products(**kwargs):\n",
    "    # Set source file\n",
    "    fs = FSHook(conn_id=\"tmp_fs\")\n",
    "    source_file_name = str(kwargs[\"execution_date\"]) + '.json'\n",
    "    source_dir_path = os.path.join(\n",
    "        fs.get_path(),\n",
    "        kwargs[\"dag\"].dag_id,\n",
    "        'extract_products')\n",
    "    target = os.path.join(source_dir_path, source_file_name)\n",
    "\n",
    "    with open(target) as json_file:\n",
    "        products = json.load(json_file)\n",
    "    \n",
    "    db = PostgresHook(postgres_conn_id=\"my_warehouse_1\")\n",
    "\n",
    "    pp(products)\n",
    "    conn = db.get_conn()\n",
    "    c = conn.cursor()\n",
    "    c.executemany('''\n",
    "        Insert into products (product_id, name) VALUES (%s,%s)\n",
    "        ON CONFLICT \n",
    "        ON CONSTRAINT products_product_id_key DO NOTHING\n",
    "    ''', products)\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "def extract_sales(**kwargs):\n",
    "    db = MySqlHook(mysql_conn_id='my_ecommerce')\n",
    "    pp(kwargs['execution_date'])\n",
    "\n",
    "    fs = FSHook(conn_id=\"tmp_fs\")\n",
    "    logging.info(\"FS path: {}\".format(fs.get_path()))\n",
    "   \n",
    "    file_name = str(kwargs[\"execution_date\"]) + '.json'\n",
    "    dir_path = os.path.join(\n",
    "        fs.get_path(),\n",
    "        kwargs[\"dag\"].dag_id,\n",
    "        kwargs[\"task\"].task_id)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    \n",
    "    target = os.path.join(dir_path, file_name)\n",
    "    \n",
    "\n",
    "    conn = db.get_conn()\n",
    "    c = conn.cursor()\n",
    "\n",
    "    c.execute('''\n",
    "        SELECT\n",
    "            oi.order_id,\n",
    "            t.id as category_id,\n",
    "            pv.product_id,\n",
    "            CAST(DATE_FORMAT(o.created_at, '%Y%m%d%H') as INTEGER) as datetime_dim,\n",
    "            CAST(sum(oi.total) as INTEGER) as total\n",
    "        FROM\n",
    "            ecommerce_order_item oi\n",
    "        JOIN \n",
    "            ecommerce_product_variant pv on pv.id = oi.variant_id\n",
    "        JOIN \n",
    "            ecommerce_order o on oi.order_id = o.id\n",
    "        JOIN\n",
    "            ecommerce_product p on p.id = pv.product_id\n",
    "        JOIN\n",
    "            ecommerce_taxon t on t.id = p.main_taxon_id\n",
    "        GROUP BY\n",
    "            oi.order_id,\n",
    "            t.id,\n",
    "            pv.product_id\n",
    "        ;\n",
    "    ''')\n",
    "\n",
    "    res = c.fetchall()\n",
    "    pp(res)\n",
    "    json.dump(res, open(target, 'w+'))\n",
    "\n",
    "def load_sales(**kwargs):\n",
    "    # Set source file\n",
    "    fs = FSHook(conn_id=\"tmp_fs\")\n",
    "    source_file_name = str(kwargs[\"execution_date\"]) + '.json'\n",
    "    source_dir_path = os.path.join(\n",
    "        fs.get_path(),\n",
    "        kwargs[\"dag\"].dag_id,\n",
    "        'extract_sales')\n",
    "    target = os.path.join(source_dir_path, source_file_name)\n",
    "\n",
    "    with open(target) as json_file:\n",
    "        products = json.load(json_file)\n",
    "    \n",
    "    db = PostgresHook(postgres_conn_id=\"my_warehouse_1\")\n",
    "\n",
    "    pp(products)\n",
    "    conn = db.get_conn()\n",
    "    c = conn.cursor()\n",
    "    c.executemany('''\n",
    "        INSERT INTO sales (order_id, product_id, category_id, datetime_id, total_sales) VALUES\n",
    "            (\n",
    "                %s,\n",
    "                (select p.id from products p where p.product_id = %s),\n",
    "                (select 1 from categories where 0 != %s LIMIT 1),\n",
    "                %s,\n",
    "                %s\n",
    "            )\n",
    "    ''', products)\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "with DAG('etl-sales', start_date=datetime(2021, 1, 10), default_args=default_args, schedule_interval=None) as dag:\n",
    "\n",
    "    start = DummyOperator(task_id='start')\n",
    "\n",
    "    extract_products = PythonOperator(task_id='extract_products', python_callable=extract_products)\n",
    "    transform_products = DummyOperator(task_id='transform_products')\n",
    "    load_products = PythonOperator(task_id='load_products', python_callable=load_products)\n",
    "\n",
    "    extract_categories = DummyOperator(task_id='extract_categories')\n",
    "    load_categories = DummyOperator(task_id='load_categories')\n",
    "\n",
    "    extract_channels = DummyOperator(task_id='extract_channels')\n",
    "    load_channels = DummyOperator(task_id='load_channels')\n",
    "\n",
    "    extract_sales = PythonOperator(task_id='extract_sales', python_callable=extract_sales)\n",
    "    load_sales = PythonOperator(task_id='load_sales', python_callable=load_sales)\n",
    "    \n",
    "    load_time = PythonOperator(\n",
    "        task_id='load_time',\n",
    "        python_callable=load_time\n",
    "    )\n",
    "\n",
    "\n",
    "    end = DummyOperator(task_id='end')\n",
    "    \n",
    "    start >> extract_products >> transform_products >> load_products\n",
    "    start >> extract_categories >> load_categories\n",
    "    start >> extract_channels >> load_channels\n",
    "    start >> extract_sales >> load_sales\n",
    "    start >> load_time\n",
    "\n",
    "    [load_products, load_time, load_categories, load_channels, load_sales] >> end\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
